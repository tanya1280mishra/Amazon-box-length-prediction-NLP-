# ğŸ“¦ Amazon Product Length Prediction Using LSTM

**Predicting box sizes from product descriptions using Deep Learning**

This project explores the use of **LSTM (Long Short-Term Memory)** networks to estimate the **physical length of Amazon product packages** based on textual information such as **titles, bullet points, and descriptions**.

> âš ï¸ *Note: Current results are not optimal. This project is a work in progress aimed at improving accuracy through better data, preprocessing, and model tuning.*

---

## ğŸ§­ Table of Contents

* [ğŸ“Š Dataset](#-dataset)
* [ğŸ§¼ Preprocessing](#-preprocessing)
* [ğŸ§  Model Architecture](#-model-architecture)
* [ğŸ“ˆ Results](#-results)
* [ğŸš€ Future Improvements](#-future-improvements)
* [âš™ï¸ Setup & Installation](#ï¸-setup--installation)
* [â–¶ï¸ Usage](#ï¸-usage)

---

## ğŸ“Š Dataset

The dataset contains **2.2 million** training samples and **734k** test entries with the following features:

| Column            | Description                                           |
| ----------------- | ----------------------------------------------------- |
| `PRODUCT_ID`      | Unique identifier                                     |
| `TITLE`           | Product name                                          |
| `BULLET_POINTS`   | Feature highlights                                    |
| `DESCRIPTION`     | Detailed product description                          |
| `PRODUCT_TYPE_ID` | Categorical product ID                                |
| `PRODUCT_LENGTH`  | (Target) Length of product box (only in training set) |

**Missing Data (Train):**

* `TITLE`: 13
* `BULLET_POINTS`: 837,366
* `DESCRIPTION`: 1,157,382

> All rows with missing values are dropped before training.

---

## ğŸ§¼ Preprocessing

To clean and standardize the text data, we apply:

1. **Drop NaNs** from critical text columns
2. **Lowercasing** all text
3. **Remove HTML tags**, emojis, URLs, and punctuation
4. **Expand contractions** (e.g., "don't" â†’ "do not")
5. **Concatenate** `BULLET_POINTS` + `DESCRIPTION` â†’ `text`
6. **Truncate** dataset to 458 rows (for quick experimentation)

> âš ï¸ Limited samples were used due to hardware constraints; full dataset training is planned.

---

## ğŸ§  Model Architecture

We use a **Sequential LSTM** model built with TensorFlow:

| Layer       | Details                                          |
| ----------- | ------------------------------------------------ |
| `Embedding` | (88,636 vocab, 200 dims, input\_len=507)         |
| `LSTM x2`   | 256 units each, one with `return_sequences=True` |
| `Dense`     | 1 unit, `linear` activation for regression       |

**Compilation:**

* `Loss`: Mean Squared Error (MSE)
* `Optimizer`: Adam
* `Metrics`: MAE, RMSE

---

## ğŸ“ˆ Results

After training for 100 epochs on the mini dataset:

| Metric       | Value          |
| ------------ | -------------- |
| **MSE**      | `6,901,454.45` |
| **RMSE**     | `2,627.06`     |
| **MAE**      | `6,901,454.45` |
| **RÂ² Score** | `-30.0152` âŒ   |

> Model output was constant (`~283.73`) indicating **no meaningful learning** on this sample size.

---

### ğŸ“‰ Visualizations

* ğŸ“ **Actual vs Predicted**: Scatter plot with dashed perfect-fit line
* ğŸ“Š **Error Distribution**: Histogram of prediction errors
* ğŸ“¦ **Predicted Lengths**: Histogram of output distribution

---

## ğŸš€ Future Improvements

Hereâ€™s how we plan to improve the model:

### ğŸ” Data

* Use **full dataset** (2.2M rows)
* Handle **missing values** more smartly (e.g., imputation)

### ğŸ§ª Preprocessing

* Add **stop word removal**, **lemmatization**, **POS tagging**
* Use **TF-IDF** or **custom tokenizer**

### ğŸ”§ Features

* Include `PRODUCT_TYPE_ID` (categorical encoding)
* Add **text length** and **readability** metrics

### ğŸ¤– Modeling

* Use **Bidirectional LSTM**, **GRU**, or **Transformer-based models** (like BERT)
* Add **Attention layers**
* Try **pretrained embeddings** (Word2Vec, GloVe, FastText)

### ğŸ“‰ Regularization

* Add **Dropout**, **L2 regularization**

### ğŸ§® Optimization

* Tune **hyperparameters** (LR, batch size, embedding dims)

---

## âš™ï¸ Setup & Installation

### 1. Clone the repo

```bash
git clone <repo-url>
cd <repo-dir>
```

### 2. Create and activate environment

```bash
python -m venv LSTMenv
source LSTMenv/bin/activate  # or LSTMenv\Scripts\activate on Windows
```

### 3. Install requirements

```bash
pip install -r requirements.txt
```

### 4. Prepare dataset

Place your `train.csv` and `test.csv` under:

```
project_root/
â”œâ”€â”€ LSTMmodel.ipynb
â””â”€â”€ dataset/
    â”œâ”€â”€ train.csv
    â””â”€â”€ test.csv
```

Update file paths in the notebook if necessary.

---

## â–¶ï¸ Usage

1. Launch Jupyter:

   ```bash
   jupyter notebook LSTMmodel.ipynb
   ```
2. Run all cells to:

   * Load and preprocess the dataset
   * Train the LSTM model
   * Evaluate performance and visualize predictions

---

## ğŸ“Œ Project Status

ğŸ”§ **Under active development** â€” working on full dataset training, feature engineering, and integration of transformer models for better performance.

---

## âœ¨ Contributions

Pull requests, ideas, and suggestions are welcome!
